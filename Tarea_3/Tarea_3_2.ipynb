{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from itertools import permutations \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicción de Juegos \n",
    "\n",
    "Un club del juego de Go recopiló los resultados de varias partidas entre diferentes jugadores, almacenados en el archivo juegos_entrenamiento.txt, con el objetivo de predecir el resultado de partidas futuras, ejemplos de las cuales se encuentran en el archivo *juegos_validacion.txt*. \n",
    "\n",
    "*archivos juegos_entrenamiento.txt* y *juegos_validacion.txt2* contienen 3 columnas: la primera corresponde al identificador del jugador A, la segunda al identificador del jugador B y la tercera es el resultado de la partida (1 si ganó el jugador A o 0 si ganó el jugador B). En el club hay un total de D jugadores, por lo que cada identificador es un número entero entre 1 y D. La predicción del resultado de un juego se puede plantear como un problema de clasificación: dados 2 jugadores (A y B) se requiere predecir si A ganó (y = 1) o si fue B (y = 0). Realice los siguientes ejercicios:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Entrena y evalúa un clasificador bayesiano ingenuo. Al ser un modelo generativo (modelala probabilidad conjunta $P(x, y)$), es posible generar partidas artificiales con los parámetros calculados. Genera nuevas partidas que sigan la distribución modelada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lectura de datos\n",
    "\n",
    "x_train = np.loadtxt(\"regl_data/juegos_entrenamiento.txt\")\n",
    "x_val = np.loadtxt(\"regl_data/juegos_validacion.txt\")\n",
    " \n",
    "d = np.shape(x_train)[0]    # Numero de jugadores \n",
    "\n",
    "nG = sum(x_train[:,-1:]==1) # Numero de Ganadores \n",
    "nP = sum(x_train[:,-1:]==0) # Numero de Perdedores  \n",
    "\n",
    "#nPA = nGB # Numero de Perdedores A\n",
    "#nPB = nGA # Numero de Perdedores B\n",
    "\n",
    "#encuentros que gano A\n",
    "JugGanA = x_train[x_train[:,-1]==1][:,0] #ID de ganadores de A\n",
    "nGA = len(JugGanA)\n",
    "JugPerB = x_train[x_train[:,-1]==1][:,1] #ID de perdedores de B\n",
    "nPB = len(JugPerB)\n",
    "#encuentros que gano B\n",
    "JugGanB = x_train[x_train[:,-1]==0][:,0] #ID de ganadores de A\n",
    "nGB = len(JugGanB)\n",
    "JugPerA = x_train[x_train[:,-1]==0][:,1] #ID de perdedores de B\n",
    "nPA = len(JugPerA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a considerar que los jugadores A y B, siguen una función de distribución categórica, dada por \n",
    "\n",
    "$$f(x;\\vec{q}) = \\prod_{k=1}^{d}q_{k}^{[x = k]}$$\n",
    "\n",
    "Donde los estimadores de la función categórica están dados por la siguiente expresión, considerando que vamos a utilizar MAP\n",
    "\n",
    "$$\\hat{q}_{k} = \\frac{c_{k}+\\alpha_{k}-1}{n+\\sum_{k=1}^{K}\\alpha_{k}+K}$$\n",
    "\n",
    "Luego se plantea el clasificador como sigue\n",
    "\n",
    "$$C= \\underset{C\\in\\{Ganar,Perder\\}}{\\mathrm{ArgMax}} \\left[ P(C)\\prod_{k=1}^{d}{(q_{A})}_{C}\\prod_{k=1}^{d}{(q_{B})}_{C}\\right] $$\n",
    "\n",
    "Donde la clase se distribuye de manera binomial , ahora se calculan los respectivos constantes\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score del modelo = 0.617\n"
     ]
    }
   ],
   "source": [
    "# Funcion que implementa el clasificador utilizando estimadores de maximo a posteriori\n",
    "\n",
    "def MAP(x,Alpha):\n",
    "    # Eleccion de alpha para tenerlo como entrada\n",
    "    K = 4\n",
    "    alpha1,alpha2,alpha3,alpha4 = Alpha[0],Alpha[1],Alpha[2],Alpha[3]\n",
    "    s1,s2,s3,s4 = alpha1*K*(K+1),alpha2*K*(K+1),alpha3*K*(K+1),alpha4*K*(K+1)\n",
    "    # Probabilidades de la clase\n",
    "    PA = nG/d ; PB = nP/d \n",
    "    \n",
    "    # Se identifica cuantas veces juega (Gana/Pierde) cada jugador  \n",
    "    qGA = np.unique(JugGanA,return_counts = True) \n",
    "    qPB = np.unique(JugPerB,return_counts = True)\n",
    "\n",
    "    qGB = np.unique(JugGanB,return_counts = True) \n",
    "    qPA = np.unique(JugPerA,return_counts = True)\n",
    "\n",
    "    # Diccionario que hace el conteo de cada jugador y lo relaciona con el numero de jugador\n",
    "    QGA = {qGA[0][i]:qGA[1][i] for i in range(len(qGA[0]))}\n",
    "    QPB = {qPB[0][i]:qPB[1][i] for i in range(len(qPB[0]))}\n",
    "\n",
    "    QGB = {qGB[0][i]:qGB[1][i] for i in range(len(qGB[0]))}\n",
    "    QPA = {qPA[0][i]:qPA[1][i] for i in range(len(qPA[0]))}\n",
    "    \n",
    "    # Implementación de la definición se debe aplicar dado que consideramos un diccionario\n",
    "    if x[0] in QGA and x[1] in QGB:  CA = float(PA*((QGA[x[0]]+alpha1-1)/(nGA+s1-K))*((QGB[x[1]]+alpha2-1)/(nGB+s2-K)))\n",
    "    if x[0] in QGA and x[1] not in QGB:  CA = float(PA*((QGA[x[0]]+alpha1-1)/(nGA+s1-K))*(alpha2-1)/(nGB+s2-K))\n",
    "    if x[0] not in QGA and x[1] in QGB: CA = float(PA*(alpha1-1/(nGA+s1-K))*((QGB[x[1]]+alpha2-1)/(nGB+s2-K)))\n",
    "    if x[0] not in QGA and x[1] not in QGB: CA = 0\n",
    "\n",
    "    if x[0] in QPA and x[1] in QPB:  CB = float(PB*((QPA[x[0]]+alpha3-1)/(nPA+s3-K))*((QPB[x[1]]+alpha4-1)/(nPB+s4-K)))\n",
    "    if x[0] in QPA and x[1] not in QPB:  CB = float(PB*((QPA[x[0]]+alpha3-1)/(nPA+s3-K))*(alpha4-1)/(nPB+s4-K))\n",
    "    if x[0] not in QPA and x[1] in QPB: CB = float(PB*(alpha3-1/(nPA+s3-K))*((QPB[x[1]]+alpha4-1)/(nPB+s4-K)))\n",
    "    if x[0] not in QPA and x[1] not in QPB: CB = 0\n",
    "    \n",
    "    clases = {float(CA):1,float(CB):0}\n",
    "        \n",
    "    return clases[max(CA,CB)]\n",
    "\n",
    "\n",
    "# Definicion de un score , verificando cuantas veces adivina el clasificador con el conjunto\n",
    "# de validacion \n",
    "\n",
    "def score(x_train,x_val):\n",
    "    s = 0\n",
    "    for i in range(len(x_val)):\n",
    "        s+=MAP(x_train[:,[0,1]][i],[2,2,0,1])==x_val[:,-1][i]\n",
    "    return s/len(x_val)\n",
    "\n",
    "# para esta eleccion particular de alpha, se tiene este score\n",
    "print(\"score del modelo = {:.3f}\".format(score(x_train,x_val)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Entrena y evalúa un clasificador de regresión logística. Para esto es necesario reparametrizar las entradas. Explica el procedimiento y la lógica de la reparametrización que realizaste. La selecciona y visualiza los valores de los parámetros.Grafica las curvas ROC y de precisiónexhaustividad y reporta sus áreas bajo la curva."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hacemos la reparametrización con variables dummy para cuaNtificar las variables categóricas del modelo , el modelo de reparamtrización que se utiliza es el más simple y común. Es una forma de convertir variables categóricas en unsa serie de variables dicotomicas. Se crea tantas variables como jugadores que participen en los encuentros, entonces se pasa de un vector de dos identificadores de jugadores a un vector de muchas componentes que solamente tiene dos componentes como uno y el resto de cero. El objetivo sera encontrar y ajustar una ecuación lineal del siguiente estilo\n",
    "\n",
    "$$y(\\vec{x}) = \\omega_{o} + \\omega_{1}x_{1}+\\omega_{2}x_{2}+\\cdots+\\omega_{M}x_{M}$$\n",
    "\n",
    "en donde $x_{i}}$ son las i-ésimas componentes de los vectores descritos anteriormente de 1 y 0. Ahora construimos una funcion que haga este procedimiento para todos los vectores que estan dentro del conjunto de entrenamiento "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(319, 141) (319, 1)\n"
     ]
    }
   ],
   "source": [
    "t = x_train[:,-1].reshape(len(x_train[:,-1]),1)\n",
    "#funcion que produce una matriz con las variables dummy de una característica \n",
    "def dummy_matrix(x):\n",
    "    # solamente se quieren tomar las columnas de los jugadores, se exluye la ultima de objetivos\n",
    "    x = x[:,:-1]\n",
    "    # Se encuentran todos los jugadores de las dos columnas de jugadores\n",
    "    J = np.unique(x)\n",
    "    # Se define el numero de característica como tantos jugadores existan\n",
    "    N = len(J)\n",
    "    # Se arma la matriz donde van a estar los vectores de 1 y 0\n",
    "    M = np.zeros((np.shape(x)[0],N-1))\n",
    "    # Ciclo que va sobre las columnas\n",
    "    for j in range(np.shape(x)[1]):\n",
    "            #ciclo que va sobre las caracter\n",
    "            for i in range(N-1):\n",
    "                index = np.where(x[:,j]==J[i])\n",
    "                M[:,i][index] = np.ones(len(index[0]))\n",
    "    M[-1] = 0\n",
    "    return M,J\n",
    "\n",
    "# Se armman la matriz que contiene las variables dummy para cada caracteristica\n",
    "D, Features = dummy_matrix(x_train)\n",
    "\n",
    "# Se considera una vector de unos de tamaño del numero de datos para armar la matriz de diseño\n",
    "unos = np.ones((np.shape(D)[0],1))\n",
    "\n",
    "print(np.shape(D),np.shape(unos))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ya se hizo la reparametrización de las variables categóricas como variables dummy, ahora se debe considerar la regresión logistica. Vemos que para dos clases, la probabilidad posterior están dadas por una función Sigmoide, dadas por \n",
    "\n",
    "$$P(C_{1}|\\phi) = y(\\phi) = \\sigma(\\vec{\\omega}^{T}\\vec{\\phi})$$\n",
    "\n",
    "Se usa la máxima verosimilitud para determinar los parametros $\\vec{\\omega}$ de este modelo directamente.\n",
    "\n",
    "Ahora se escribe la función de probabilidad usando un esquema $1-K$ con el vector objetivo $\\vec{t}_{n}$ para un vector de características $\\vec{\\phi}_{n}$ perteneciente a la clase  \n",
    "\n",
    "$$P(t|w) = \\prod_{n=1}^{N}y_{n}^{t_{n}}(1-y_{n})^{1-t_{n}}$$\n",
    "\n",
    "Donde $\\vec{t} = (t_{1},\\cdots,t_{N})$ y $y_{n} = P(C_{1}|\\phi_{n})$\n",
    "\n",
    "\n",
    "Tomando el logaritmo negativo se tiene la llamada entropía-cruzada, que es la función de error para la clasificación es\n",
    "\n",
    "$$E(\\vec{\\omega})=-\\ln P(\\vec{t_{1}},\\cdots,\\vec{t_{N}}|\\vec{\\omega})=-\\sum_{n=1}^{N}[t_{n}\\ln y{n} +(1-t_{n})\\ln (1-y_{n})]$$\n",
    "\n",
    "Ahora el gradiente de esta expresión se puede deducir de la siguiente manera donde \n",
    "\n",
    "\n",
    "$$\\nabla_{\\vec{\\omega}}E(\\vec{w})=\\sum_{n=1}^{N}(y_{n}-t_{n})\\vec{\\phi}$$\n",
    "\n",
    "Ahora se necesita una manera de encontrar la actualización del vector de parámetros, como $E(\\vec{\\omega})$ es convexa se puede asegurar que la función tiene un minimo global único, para encontrar el paso de actualización se podría utilizar el método de Newton-Raphson para minimizar cualquier función está dado por \n",
    "\n",
    "$$\\vec{\\omega}^{\\tau+1} = \\vec{\\omega}^{\\tau} - \\mathcal{H}^{-1}\\nabla_{\\omega}E(\\vec{\\omega})$$\n",
    "\n",
    "Donde $\\mathcal{H}$ es la matriz Hessiana compuesta de las segundas derivadas de la función de error, que por definición está dada por \n",
    "\n",
    "$$\\mathcal{H} = \\nabla_{\\omega}\\nabla_{\\omega}E(\\vec{\\omega}) = \\sum_{n=1}^{N}y_{n}(1-y_{n})\\phi_{n}\\phi_{n^{T}}$$\n",
    "\n",
    "notamos que $\\mathcal{H}$ depende del estado actual de $\\vec{\\omega}^{T}$ \n",
    "\n",
    "Sin embargo vamos a optar por el método más simple para poder calcular la actualización de $\\vec{\\omega}$, dado que el anterior método nos puede llevar a problemas numéricos en el calculo de la inversa de la matriz que no se quieren lidiar ahora. El metodo será el descenso más rápido \"Steepest descent\", quizás el algoritmo más simple para la optimización sin restricciones es el descenso de gradiente. Cuya regla de actualización se escribe como \n",
    "\n",
    "$$\\vec{\\omega}^{\\tau+1} = \\vec{\\omega}^{\\tau} - \\eta_{\\tau}\\nabla_{\\omega}E(\\vec{\\omega})$$\n",
    "\n",
    "donde $\\eta_{\\tau}$ es la que denominaremos tasa de aprendizaje. El principal problema de este método es determinar un valor óptimo para $\\eta$. En este caso se considera una tasa constante de aprendizaje, de manera arbitraria, que se elige de acuerdo a que el resultado de adecuado o por lo menos no tan descabellado. Es importante no elegirlo tan grande , comparado con la unidad, esto porque como se tiene una funcion de error concava al elegir un valor grande va a quedar \"rebotando\" el valor del error en cada paso sin posibilidad de llegar a un mínimo, sin embargo cuando se elige un adecuado \"pequeño\" se tiene un descenso suave a traves de la función de error\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para evadir el problema del zig-zag una solución heuristicaes adicionar un término de momento $w_{\\tau}-w_{\\tau-1}$ como sigue\n",
    "\n",
    "$$\\vec{\\omega}^{\\tau+1} = \\vec{\\omega}^{\\tau} - \\eta_{\\tau}\\nabla_{\\omega}E(\\vec{\\omega}) + \\mu_{k}(\\vec{w}^{\\tau}-\\vec{w}^{\\tau-1})$$\n",
    "\n",
    "donde $0\\leq \\mu \\leq 1$ controla la importancia del nuevo termino. Es conocido como *método de bola pesada* (heavy ball method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definamos la funcion sigmoide\n",
    "def sigmoid(z):\n",
    "    return 1/(1+np.exp(-z))\n",
    "\n",
    "# Definimos la funcion de pérdida para poder medir el error en cada iteracion cuando se este optimizando los valores de w\n",
    "def Loss(y,y_pred):\n",
    "    return np.sum((y_pred - y)**2)\n",
    "\n",
    "# Construimos la matriz de diseño\n",
    "unos = np.ones((np.shape(D)[0],1))\n",
    "Phi = np.concatenate((unos,D),axis = 1)\n",
    "\n",
    "# Debemos encontrar parametros de la linealizacion creamos un vector de numeros aleatorios que corresponda con el numero de variables \n",
    "w = np.random.uniform(0,0.01,(1,np.shape(Phi)[1])).T\n",
    "# Inicializamos una variable auxiliar para el calculo de w\n",
    "w_prov = 0\n",
    "# Definimos la lista donde guardamos la funcion de error a cada paso\n",
    "lost = []\n",
    "# Definimos la tasa de aprendizaje\n",
    "η = 0.01\n",
    "# Control del momento \n",
    "μ = 0.1\n",
    "\n",
    "#Ciclo de actualizacion w\n",
    "aux = 1\n",
    "while True:\n",
    "    # Calculo de y\n",
    "    Y = sigmoid(w.T@Phi.T).T\n",
    "    #Calculamos el gradiente\n",
    "    G = Phi.T@(Y-t)\n",
    "    #regla de actuaizacion\n",
    "    w_prov = w - η*G + μ*(w-w_prov)\n",
    "    # Calculo de la prediccion\n",
    "    Y_prov = sigmoid(w_prov.T@Phi.T).T \n",
    "    ver = Loss(Y_prov,t)#np.sum((Y_prov - t)**2)\n",
    "    # Condicion para dejar de calcular \n",
    "    if abs(ver - aux)< 1e-7:\n",
    "        break\n",
    "    lost.append(ver)\n",
    "    w = w_prov\n",
    "    aux = ver\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veamos el descenso de la función de pérrdida "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error alcanzado = 29.87175035750377\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0, 5000)"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10,8))\n",
    "plt.plot(lost,\"b--\")\n",
    "plt.xlabel(\"Pasos\")\n",
    "plt.ylabel(\"Error\")\n",
    "plt.title(\"Descenso\", size = 12)\n",
    "plt.grid(True)\n",
    "print(\"Error alcanzado = {}\".format(lost[-1]))\n",
    "plt.xlim(0,5000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez encontrado los coeficientes debemos prepararlo para hacer predicciones dado dos jugadores cualquiera , para esto debemos construir funciones que pongan el identificador del jugador en terminos de la base de vectores dummy que anteriormente creamos \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6521739130434783"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_val = x_val[:,-1]\n",
    "x = x_train[:,:-1]\n",
    "J = np.unique(x)\n",
    "N = len(J)\n",
    "L = np.zeros((1,N))\n",
    "\n",
    "pred = []\n",
    "for i in range(len(x_val)):\n",
    "    Jug1, Jug2 = x_val[i][:-1]\n",
    "    L = np.zeros((1,N))\n",
    "    index = np.where(J == Jug1)\n",
    "    index2 = np.where(J == Jug2) \n",
    "    L[0,index] = 1\n",
    "    L[0,index2] = 1\n",
    "    #L = dummy_Jug(Jug1,Jug2)\n",
    "    a = sigmoid(w.T@L.T)[0,0]\n",
    "    pred.append(a)\n",
    "    \n",
    "pred = np.array(pred)\n",
    "probC1 = pred\n",
    "probC2 = 1-pred\n",
    "t_pred = []\n",
    "for i in range(len(pred)):\n",
    "    if probC1[i] > probC2[i]:\n",
    "        t_pred.append(0)\n",
    "    else:\n",
    "        t_pred.append(1)\n",
    "t_pred = np.array(t_pred)\n",
    "\n",
    "score = sum(t_pred == t_val)/len(t_pred)\n",
    "score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Compara el clasificador bayesiano ingenuo y regresión logística en este problema. ¿Qué ventajas y desventajas tienen los modelos entrenados? ¿Qué pasaría si se entrena el clasificador bayesiano ingenuo con los vectores reparametrizados o si se entrena un modelo de regresión logística usando los vectores de entrada originales? ¿Consideras que las presuposiciones de cada clasificador son apropiadas para los datos del problema? ¿Para este tipo de problemas cuál de los dos recomendarías y por qué?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Naive Bayes** es un método de clasificación basado en el teorema de Bayes que deriva la probabilidad de que el vector de características dado esté asociado con una etiqueta. Tiene una suposicion \"ingenua\" de independencia condicional para cada característica, es decir que algoritmo espera que las características sean independientes, cosa que no ocurre siempre. la **Regresión logística** es un método de clasificacion lineal que aprende la probabilidad de que una muestra pertenezca a una determinada clase. Intenta encontral el límite de decisión óptimo que se pare mejor las clases\n",
    "\n",
    "#### 1. Ambos se utilizan para la clasificación\n",
    "\n",
    "#### 2. Mecanismo de aprendizaje\n",
    "\n",
    "El arendizaje en cada método es un poco diferente , **Naive Bayes** es un modelo generativo y la **Regresión logística** es un modelo discriminativo \n",
    "\n",
    "#### 3. Suposiciones del modelo\n",
    "\n",
    "**Naive Bayes** asume que todas las características son condicionalmente independientes. Por lo tano, si alguna de las características depende entre si la predicción podría ser deficiente.\n",
    "\n",
    "**Regresión logística** hereda muchas suposiciones del ajuste lineal corriente, por tanto está construido sobre la suposiciónde independencia lineal y funcionan razonablemente aun cuando estén debilmente correlacionadas las variables independientes\n",
    "\n",
    "#### 4. ¿Qué condiciones mejoran el rendimiento ?\n",
    "\n",
    "**Naive Bayes:** cuando el tamaño de los datos de entrenamiento es pequeño en relación con el número de características, la relación (informacion/datos) en las probabilidades apriori ayudan a mejorar los resultado\n",
    "\n",
    "**Regresión logística:** Cuando el tamaño de los datos de entrenamiento es pequeño relativo al número de características, se debe utilizar regularización para evitar problemas de sobreajuste \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cuando se reparametriza las variables categóricas se incrementan drásticamente el numero de características que el modelo debe considerar, en el caso del clasificador Bayesiano ingenuo será mucho más costoso el calculo de los estimadores dado que debe hacer una estimación para cada una de las características. Si se ingresan las variables de los jugadores en un modelo de regresión logística va a operar bajo el valor numérico que identifica al jugador y eso no tienen sentido, dado que el numero que se consigna en los datos es un nuero que identifica al jugador y no una característica propia del sistema."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En particular recomendaría pensar en modelos discriminativos **regresión logística **para este problema, dado que debemos suponer menos respecto de los jugadores, cuando se utiliza el calsificador ingenuo se asume una probabilidad apriori tratando de muestrear correctamente los datos, es una propuesta que se hace partiendo de lo que suponemos del sistema, de alguna manera se puede estar violando, bajo la particular elección de la distribución de probabilidad, la independencia condicional de las características y esto puede llevar a malos resultados. Aunque se sabe que el clasificador Bayesiano alcanza el límite asintótico más rápido está más sesgado que la regresión logística y otro punto importante es que en la regresión logística se puede implementar la penalización por norma, esto le posibilita al sistema poder generalizar mejor las predicciones del modelo "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Deriva la regla de actualización para el algoritmo del descenso por gradiente de un clasificador donde $\\hat{y} = \\text{sigm}(\\vec{\\theta}^{T}\\vec{x})$ y la función de pérdida sea\n",
    "\n",
    "$$E(\\vec{\\theta}) = \\frac{1}{2}\\sum_{i=1}^{n}(\\hat{y}^{(i)}-y^{(i)})$$\n",
    "\n",
    " \n",
    "#### Estimación del vector $\\vec{\\omega}$ \n",
    "\n",
    "La probabilidad de un evento se describe por \n",
    "\n",
    "$$P(\\vec{y}|\\vec{x},\\vec{\\omega}) = \\prod_{n=1}^{N}P(y_{n}|\\vec{x}_{n},\\vec{\\omega}) = \\prod_{n=1}^{N}\\mu_{n}^{y_{n}}(1-\\mu_{n})^{1-y_{n}}\\qquad \\mu_{n} = \\frac{e^{\\vec{\\omega}^{T}\\vec{x}_{n}}}{1+e^{\\vec{\\omega}^{T}\\vec{x}_{n}}}$$\n",
    "\n",
    "Tomando el logaritmo se llega a la siguiente definición\n",
    "\n",
    "$$NLL(\\vec{\\omega}) = -\\sum_{n=1}^{N}\\left[y_{n}\\log \\mu_{n}+(1-y_{n})\\log (1-\\mu_{n})\\right]$$\n",
    "\n",
    "$$NLL(\\vec{\\omega}) = -\\sum_{n=1}^{N}\\left[y_{n}\\vec{\\omega}^{T}\\vec{x}_{n}-\\log(1-e^{\\vec{\\omega}^{T}\\vec{x}_{n}})\\right]$$\n",
    "\n",
    "Ahora se toma la derivada\n",
    "\n",
    "$$\\vec{g} = \\frac{\\partial(NLL(\\vec{\\omega}))}{\\partial \\vec{\\omega}} =\\frac{\\partial}{\\partial \\vec{\\omega}}\\left[\\sum_{n=1}^{N}(y_{n}\\vec{\\omega^{t}}\\vec{x}_{n})-\\log (1-e^{\\vec{\\omega}^{t}\\vec{x}_{n}})\\right]$$\n",
    "\n",
    "$$\\vec{g} = -\\sum_{n=1}^{N}\\left(y_{n}\\vec{x}_{n}-\\frac{e^{(\\vec{\\omega}^{T}\\vec{x}_{n})}}{1+e^{\\vec{\\omega}^{T}\\vec{x}_{n}}}\\vec{x}_{n}\\right)$$\n",
    "\n",
    "$$\\vec{g} = -\\sum_{n=1}^{N}(y_{n}-\\mu_{n})\\vec{x}_{n}$$\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No tenemos una solución analítica del tipo $\\vec{g} = 0$, entonces vemos que la convexidad de la función asegura un mínimo global, por lo tanto vamos a buscar ese mínimo haciendo que el vector descienda en la dirección de mayor crecimiento, esto se hace de la siguiente manera\n",
    "\n",
    "$$\\vec{\\omega}^{\\tau+1} = \\vec{\\omega}^{\\tau}-\\eta\\sum_{n=1}^{N}(\\mu_{n}^{\\tau}-y_{n})\\vec{x}_{n} \\qquad \\mu_{n}^{\\tau} = \\text{Sigm}((\\omega^{\\tau})^{T}\\vec{x}_{n}) $$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
