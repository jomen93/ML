{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from itertools import permutations \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicción de Juegos \n",
    "\n",
    "Un club del juego de Go recopiló los resultados de varias partidas entre diferentes jugadores, almacenados en el archivo juegos_entrenamiento.txt, con el objetivo de predecir el resultado de partidas futuras, ejemplos de las cuales se encuentran en el archivo *juegos_validacion.txt*. \n",
    "\n",
    "*archivos juegos_entrenamiento.txt* y *juegos_validacion.txt2* contienen 3 columnas: la primera corresponde al identificador del jugador A, la segunda al identificador del jugador B y la tercera es el resultado de la partida (1 si ganó el jugador A o 0 si ganó el jugador B). En el club hay un total de D jugadores, por lo que cada identificador es un número entero entre 1 y D. La predicción del resultado de un juego se puede plantear como un problema de clasificación: dados 2 jugadores (A y B) se requiere predecir si A ganó (y = 1) o si fue B (y = 0). Realice los siguientes ejercicios:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Entrena y evalúa un clasificador bayesiano ingenuo. Al ser un modelo generativo (modelala probabilidad conjunta $P(x, y)$), es posible generar partidas artificiales con los parámetros calculados. Genera nuevas partidas que sigan la distribución modelada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lectura de datos\n",
    "\n",
    "x_train = np.loadtxt(\"regl_data/juegos_entrenamiento.txt\")\n",
    "x_val = np.loadtxt(\"regl_data/juegos_validacion.txt\")\n",
    " \n",
    "d = np.shape(x_train)[0]    # Numero de jugadores \n",
    "\n",
    "nG = sum(x_train[:,-1:]==1) # Numero de Ganadores \n",
    "nP = sum(x_train[:,-1:]==0) # Numero de Perdedores  \n",
    "\n",
    "#nPA = nGB # Numero de Perdedores A\n",
    "#nPB = nGA # Numero de Perdedores B\n",
    "\n",
    "#encuentros que gano A\n",
    "JugGanA = x_train[x_train[:,-1]==1][:,0] #ID de ganadores de A\n",
    "nGA = len(JugGanA)\n",
    "JugPerB = x_train[x_train[:,-1]==1][:,1] #ID de perdedores de B\n",
    "nPB = len(JugPerB)\n",
    "#encuentros que gano B\n",
    "JugGanB = x_train[x_train[:,-1]==0][:,0] #ID de ganadores de A\n",
    "nGB = len(JugGanB)\n",
    "JugPerA = x_train[x_train[:,-1]==0][:,1] #ID de perdedores de B\n",
    "nPA = len(JugPerA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a considerar que los jugadores A y B, siguen una función de distribución categórica, dada por \n",
    "\n",
    "$$f(x;\\vec{q}) = \\prod_{k=1}^{d}q_{k}^{[x = k]}$$\n",
    "\n",
    "Donde los estimadores de la función categórica están dados por la siguiente expresión, considerando que vamos a utilizar MAP\n",
    "\n",
    "$$\\hat{q}_{k} = \\frac{c_{k}+\\alpha_{k}-1}{n+\\sum_{k=1}^{K}\\alpha_{k}+K}$$\n",
    "\n",
    "Luego se plantea el clasificador como sigue\n",
    "\n",
    "$$C= \\underset{C\\in\\{Ganar,Perder\\}}{\\mathrm{ArgMax}} \\left[ P(C)\\prod_{k=1}^{d}{(q_{A})}_{C}\\prod_{k=1}^{d}{(q_{B})}_{C}\\right] $$\n",
    "\n",
    "Donde la clase se distribuye de manera binomial , ahora se calculan los respectivos constantes\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score del modelo = 0.617\n"
     ]
    }
   ],
   "source": [
    "# Funcion que implementa el clasificador utilizando estimadores de maximo a posteriori\n",
    "\n",
    "def MAP(x,Alpha):\n",
    "    # Eleccion de alpha para tenerlo como entrada\n",
    "    K = 4\n",
    "    alpha1,alpha2,alpha3,alpha4 = Alpha[0],Alpha[1],Alpha[2],Alpha[3]\n",
    "    s1,s2,s3,s4 = alpha1*K*(K+1),alpha2*K*(K+1),alpha3*K*(K+1),alpha4*K*(K+1)\n",
    "    # Probabilidades de la clase\n",
    "    PA = nG/d ; PB = nP/d \n",
    "    \n",
    "    # Se identifica cuantas veces juega (Gana/Pierde) cada jugador  \n",
    "    qGA = np.unique(JugGanA,return_counts = True) \n",
    "    qPB = np.unique(JugPerB,return_counts = True)\n",
    "\n",
    "    qGB = np.unique(JugGanB,return_counts = True) \n",
    "    qPA = np.unique(JugPerA,return_counts = True)\n",
    "\n",
    "    # Diccionario que hace el conteo de cada jugador y lo relaciona con el numero de jugador\n",
    "    QGA = {qGA[0][i]:qGA[1][i] for i in range(len(qGA[0]))}\n",
    "    QPB = {qPB[0][i]:qPB[1][i] for i in range(len(qPB[0]))}\n",
    "\n",
    "    QGB = {qGB[0][i]:qGB[1][i] for i in range(len(qGB[0]))}\n",
    "    QPA = {qPA[0][i]:qPA[1][i] for i in range(len(qPA[0]))}\n",
    "    \n",
    "    # Implementación de la definición se debe aplicar dado que consideramos un diccionario\n",
    "    if x[0] in QGA and x[1] in QGB:  CA = float(PA*((QGA[x[0]]+alpha1-1)/(nGA+s1-K))*((QGB[x[1]]+alpha2-1)/(nGB+s2-K)))\n",
    "    if x[0] in QGA and x[1] not in QGB:  CA = float(PA*((QGA[x[0]]+alpha1-1)/(nGA+s1-K))*(alpha2-1)/(nGB+s2-K))\n",
    "    if x[0] not in QGA and x[1] in QGB: CA = float(PA*(alpha1-1/(nGA+s1-K))*((QGB[x[1]]+alpha2-1)/(nGB+s2-K)))\n",
    "    if x[0] not in QGA and x[1] not in QGB: CA = 0\n",
    "\n",
    "    if x[0] in QPA and x[1] in QPB:  CB = float(PB*((QPA[x[0]]+alpha3-1)/(nPA+s3-K))*((QPB[x[1]]+alpha4-1)/(nPB+s4-K)))\n",
    "    if x[0] in QPA and x[1] not in QPB:  CB = float(PB*((QPA[x[0]]+alpha3-1)/(nPA+s3-K))*(alpha4-1)/(nPB+s4-K))\n",
    "    if x[0] not in QPA and x[1] in QPB: CB = float(PB*(alpha3-1/(nPA+s3-K))*((QPB[x[1]]+alpha4-1)/(nPB+s4-K)))\n",
    "    if x[0] not in QPA and x[1] not in QPB: CB = 0\n",
    "    \n",
    "    clases = {float(CA):1,float(CB):0}\n",
    "        \n",
    "    return clases[max(CA,CB)]\n",
    "\n",
    "\n",
    "# Definicion de un score , verificando cuantas veces adivina el clasificador con el conjunto\n",
    "# de validacion \n",
    "\n",
    "def score(x_train,x_val):\n",
    "    s = 0\n",
    "    for i in range(len(x_val)):\n",
    "        s+=MAP(x_train[:,[0,1]][i],[2,2,0,1])==x_val[:,-1][i]\n",
    "    return s/len(x_val)\n",
    "\n",
    "# para esta eleccion particular de alpha, se tiene este score\n",
    "print(\"score del modelo = {:.3f}\".format(score(x_train,x_val)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora se juega un poco con los valores de $\\alpha$ para esto hacemos grupos de 4 teniendo en cuenta los numeros de 1 a 10 para encontrar en esta pequeña prueba el valor que maximiza nuestra predicción"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.6521739130434783, (9, 3, 0, 1)], dtype=object)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perm = list(permutations(np.arange(10), 4))\n",
    "\n",
    "#buscamos unos valores un poco mejores para el modelo\n",
    "S = []\n",
    "for j in perm:\n",
    "    s = 0\n",
    "    for i in range(len(x_val)):\n",
    "        s+=MAP(x_train[:,[0,1]][i],j)==x_val[:,-1][i]\n",
    "    S.append([s/len(x_val),j])\n",
    "S = np.array(S)\n",
    "\n",
    "S[np.argmax(S[:,0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Entrena y evalúa un clasificador de regresión logística. Para esto es necesario reparametrizar las entradas. Explica el procedimiento y la lógica de la reparametrización que realizaste. La selecciona y visualiza los valores de los parámetros.Grafica las curvas ROC y de precisiónexhaustividad y reporta sus áreas bajo la curva."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hacemos la reparametrización con variables dummy para cunatificar las variables categóricas del modelo "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "JugadorA = x_train[:,0]\n",
    "JugadorB = x_train[:,1]\n",
    "t = x_train[:,-1].reshape(len(JugadorA),1)\n",
    "\n",
    "N = len(JugadorA)\n",
    "#funcion que produce una matriz con las variables dummy de una característica \n",
    "def dummy_matrix(x):\n",
    "    J = np.unique(x)\n",
    "    N = len(J)\n",
    "    M = np.zeros((len(x),N-1))\n",
    "    for i in range(N-1):\n",
    "        index = np.where(x==J[i])\n",
    "        M[:,i][index] = np.ones(len(index[0]))\n",
    "    M[-1] = 0\n",
    "    return M\n",
    "\n",
    "# Se armman las matrices que contienen las variables dummy para cada caracteristica, sin embargo son menos los jugadores de B que los que hay en A\n",
    "# por tanto debemos armar la matriz con el tamaño de jugadores que hay en A\n",
    "\n",
    "NumJA = len(np.unique(JugadorA))\n",
    "NumJB = len(np.unique(JugadorB))\n",
    "\n",
    "# Se considera una matriz cuadrada de ceros de tamaño del que más jugadores tenga, para empezar a armar la matriz de variables\n",
    "unos = np.ones((1,N)).T\n",
    "\n",
    "#Ahora se deben rellenar con las variables Dummy, por definicion tiene el tamaño de una de las matrices dummy, por lo cual no debería tener problemas con la primera\n",
    "#caracteristica. El problema esta con la segunda característica, vamos e rellenar con ceros lo que le hace falta para tener el mismo tamaño que la matriz de la \n",
    "#primera característica\n",
    "\n",
    "#Calculamos las matrices dummy\n",
    "DA = dummy_matrix(JugadorA)\n",
    "DB = dummy_matrix(JugadorB)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ya se hizo la reparametrización de las variables categóricas como variables dummy, ahora se debe considerar la regresión logistica. Vemos que para dos clases, la probabilidad posterior están dadas por una función Sigmoide, dadas por \n",
    "\n",
    "$$P(C_{1}|\\phi) = y(\\phi) = \\sigma(\\vec{\\omega}^{T}\\vec{\\phi})$$\n",
    "\n",
    "Se usa la máxima verosimilitud para determinar los parametros $\\vec{\\omega}$ de este modelo directamente.\n",
    "\n",
    "Ahora se escribe la función de probabilidad usando un esquema $1-K$ con el vector objetivo $\\vec{t}_{n}$ para un vector de características $\\vec{\\phi}_{n}$ perteneciente a la clase  \n",
    "\n",
    "$$P(t|w) = \\prod_{n=1}^{N}y_{n}^{t_{n}}(1-y_{n})^{1-t_{n}}$$\n",
    "\n",
    "Donde $\\vec{t} = (t_{1},\\cdots,t_{N})$ y $y_{n} = P(C_{1}|\\phi_{n})$\n",
    "\n",
    "\n",
    "Tomando el logaritmo negativo se tiene la llamada entropía-cruzada, que es la función de error para la clasificación es\n",
    "\n",
    "$$E(\\vec{\\omega})=-\\ln P(\\vec{t_{1}},\\cdots,\\vec{t_{N}}|\\vec{\\omega})=-\\sum_{n=1}^{N}[t_{n}\\ln y{n} +(1-t_{n})\\ln (1-y_{n})]$$\n",
    "\n",
    "Ahora el gradiente de esta expresión se puede deducir de la siguiente manera donde \n",
    "\n",
    "\n",
    "$$\\nabla_{\\vec{\\omega}}E(\\vec{w})=\\sum_{n=1}^{N}(y_{n}-t_{n})\\vec{\\phi}$$\n",
    "\n",
    "Ahora se necesita una manera de encontrar la actualización del vector de parámetros, como $E(\\vec{\\omega})$ es convex se puede asegurar que la función tiene un minimo global único, para encontrar el paso de actualización dse utiliza el método de Newton-Raphson para minimizar cualquier función está dado por \n",
    "\n",
    "$$\\vec{\\omega}^{\\tau+1} = \\vec{\\omega}^{\\tau} - \\mathcal{H}^{-1}\\nabla_{\\omega}E(\\vec{\\omega})$$\n",
    "\n",
    "Donde $\\mathcal{H}$ es la matriz Hessiana compuesta de las segundas derivadas de la función de error, que por definición está dada por \n",
    "\n",
    "$$\\mathcal{H} = \\nabla_{\\omega}\\nabla_{\\omega}E(\\vec{\\omega}) = \\sum_{n=1}^{N}y_{n}(1-y_{n})\\phi_{n}\\phi_{n^{T}}$$\n",
    "\n",
    "notamos que $\\mathcal{H}$ depende del estado actual de $\\vec{\\omega}^{T}$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usando la notación de matriz encontramos las cantidades que tenemos que encontrar, dadas por\n",
    "\n",
    "$$\\nabla_{\\vec{\\omega}}E(\\vec{w}) = \\Phi^{T}(\\vec{y}-\\vec{t})$$\n",
    "\n",
    "donde $\\Phi$ es la matriz de diseño $N\\times M$, $\\vec{y}$ es el vector de salida , que utiliza la función sigmoide y es similar al vector $\\vec{t}$ que es el vector de objetivos del sistema\n",
    "\n",
    "La matriz $\\mathcal{H}$ puede ser escrita como sigue \n",
    "\n",
    "$$\\mathcal{H} = \\Phi^{T}\\mathcal{R}\\Phi$$\n",
    "\n",
    "donde $\\mathcal{R}$ es una matriz diagonal de $N\\times N$ con elementos \n",
    "\n",
    "$$\\mathcal_{R}_{nn} = y{n}(1-y_{n})$$\n",
    "\n",
    "se interpreta como la matriz \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.66442736061291e-99"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ahora armamos la matriz que representa las variables dummy \n",
    "D = np.concatenate((DA,DB),axis=1)\n",
    "\n",
    "#Definamos la funcion sigmoide\n",
    "def sigmoid(z):\n",
    "    return 1/(1+np.exp(-z))\n",
    "\n",
    "#Construimos la matriz de diseño\n",
    "unos = np.ones((np.shape(D)[0],1))\n",
    "Phi = np.concatenate((unos,D),axis = 1)\n",
    "\n",
    "# debemos encontrar parametros de la linealizacion creamos un vector de numeros aleatorios\n",
    "# que corresponda con el numero de variables \n",
    "w = np.random.uniform(0,0.001,(1,np.shape(Phi)[1])).T\n",
    "\n",
    "#calculamos y\n",
    "Y = sigmoid(w.T@Phi.T).T\n",
    "\n",
    "#Calculamos el gradiente\n",
    "G = Phi.T@(Y-t)\n",
    "\n",
    "# calculamos la matriz Hessiana, pero primero se construye la matriz de covarianza\n",
    "r = np.zeros((len(JugadorA),len(JugadorA)))\n",
    "np.fill_diagonal(r,Y*(1-Y))\n",
    "H = Phi.T@r@Phi\n",
    "\n",
    "# Luego la actualización por Neewton-Rhapson\n",
    "\n",
    "np.linalg.det(H[0:150,0:150])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Compara el clasificador bayesiano ingenuo y regresión logística en este problema. ¿Qué ventajas y desventajas tienen los modelos entrenados? ¿Qué pasaría si se entrena el clasificador bayesiano ingenuo con los vectores reparametrizados o si se entrena un modelo de regresión logística usando los vectores de entrada originales? ¿Consideras que las presuposiciones de cada clasificador son apropiadas para los datos del problema? ¿Para este tipo de problemas cuál de los dos recomendarías y por qué?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Deriva la regla de actualización para el algoritmo del descenso por gradiente de un clasificador donde $\\hat{y} = \\text{sigm}(\\vec{\\theta}^{T}\\vec{x})$ y la función de pérdida sea\n",
    "\n",
    "$$E(\\vec{\\theta}) = \\frac{1}{2}\\sum_{i=1}^{n}(\\hat{y}^{(i)}-y^{(i)})$$\n",
    "\n",
    "  Discute las diferencias entre este clasificador y el de regresión logística y compara sus rendimientos en la tarea de predicción de juegos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
